
step.modules.transformer
========================

.. py:module:: step.modules.transformer


Overview
--------

.. list-table:: Classes
   :header-rows: 0
   :widths: auto
   :class: summarytable

   * - :py:obj:`PreNorm <step.modules.transformer.PreNorm>`
     - Base class for all neural network modules.
   * - :py:obj:`FeedForward <step.modules.transformer.FeedForward>`
     - Base class for all neural network modules.
   * - :py:obj:`Attention <step.modules.transformer.Attention>`
     - Base class for all neural network modules.
   * - :py:obj:`Transformer <step.modules.transformer.Transformer>`
     - Base class for all neural network modules.


.. list-table:: Function
   :header-rows: 0
   :widths: auto
   :class: summarytable

   * - :py:obj:`exists <step.modules.transformer.exists>`\ (val)
     - \-
   * - :py:obj:`pair <step.modules.transformer.pair>`\ (t)
     - \-
   * - :py:obj:`softmax_one <step.modules.transformer.softmax_one>`\ (x, dim, _stacklevel, dtype)
     - \-



Classes
-------

.. py:class:: PreNorm(dim, fn)

   Bases: :py:obj:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   Initializes internal Module state, shared by both nn.Module and ScriptModule.


   .. rubric:: Overview


   .. list-table:: Methods
      :header-rows: 0
      :widths: auto
      :class: summarytable

      * - :py:obj:`forward <step.modules.transformer.PreNorm.forward>`\ (x, \*\*kwargs)
        - \-


   .. rubric:: Members

   .. py:method:: forward(x, **kwargs)




.. py:class:: FeedForward(dim, hidden_dim, dropout=0.0)

   Bases: :py:obj:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   Initializes internal Module state, shared by both nn.Module and ScriptModule.


   .. rubric:: Overview


   .. list-table:: Methods
      :header-rows: 0
      :widths: auto
      :class: summarytable

      * - :py:obj:`forward <step.modules.transformer.FeedForward.forward>`\ (x)
        - \-


   .. rubric:: Members

   .. py:method:: forward(x)




.. py:class:: Attention(dim, heads=8, dim_head=64, dropout=0.0)

   Bases: :py:obj:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   Initializes internal Module state, shared by both nn.Module and ScriptModule.


   .. rubric:: Overview


   .. list-table:: Methods
      :header-rows: 0
      :widths: auto
      :class: summarytable

      * - :py:obj:`forward <step.modules.transformer.Attention.forward>`\ (x)
        - \-


   .. rubric:: Members

   .. py:method:: forward(x)




.. py:class:: Transformer(dim, depth, heads, dim_head, mlp_dim, dropout=0.0)

   Bases: :py:obj:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   Initializes internal Module state, shared by both nn.Module and ScriptModule.


   .. rubric:: Overview


   .. list-table:: Methods
      :header-rows: 0
      :widths: auto
      :class: summarytable

      * - :py:obj:`forward <step.modules.transformer.Transformer.forward>`\ (x)
        - \-


   .. rubric:: Members

   .. py:method:: forward(x)




Functions
---------
.. py:function:: exists(val)


.. py:function:: pair(t)


.. py:function:: softmax_one(x, dim=None, _stacklevel=3, dtype=None)





